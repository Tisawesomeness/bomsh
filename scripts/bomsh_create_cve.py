#! /bin/env python3
# Copyright (c) 2022 Cisco and/or its affiliates.
#
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at:
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Bomsh script to create CVE database for git repository of software.

Complementary to the gitBOM artifact tree generated by the bomsh_hook script
which works under Bomsh or Bomtrace.
Used by the bomsh_search_cve script to find CVE info of released software binaries.

December 2021, Yongkui Han
"""

import argparse
import sys
import os
import shutil
import subprocess
import json
import pdb
import re

# for special filename handling with shell
try:
    from shlex import quote as cmd_quote
except ImportError:
    from pipes import quote as cmd_quote

TOOL_VERSION = '0.0.1'
VERSION = '%(prog)s ' + TOOL_VERSION

LEVEL_0 = 0
LEVEL_1 = 1
LEVEL_2 = 2
LEVEL_3 = 3
LEVEL_4 = 4

args = None

g_logfile = "/tmp/bomsh_create_cve_logfile"
g_jsonfile = "/tmp/bomsh_created_cvedb.json"
g_all_zero_checksum = '0' * 40

#
# Helper routines
#########################
def verbose(string, level=1, logfile=None):
    """
    Prints information to stdout depending on the verbose level.
    :param string: String to be printed
    :param level: Unsigned Integer, listing the verbose level
    :param logfile: file to write
    """
    if args.verbose >= level:
        if logfile:
            append_text_file(logfile, string + "\n")
        # also print to stdout
        print(string)


def write_text_file(afile, text):
    '''
    Write a string to a text file.

    :param afile: the text file to write
    '''
    with open(afile, 'w') as f:
         return f.write(text)


def append_text_file(afile, text):
    '''
    Append a string to a text file.

    :param afile: the text file to write
    '''
    with open(afile, 'a+') as f:
         return f.write(text)


def read_text_file(afile):
    '''
    Read a text file as a string.

    :param afile: the text file to read
    '''
    with open(afile, 'r') as f:
         return (f.read())


def get_shell_cmd_output(cmd):
    """
    Returns the output of the shell command "cmd".

    :param cmd: the shell command to execute
    """
    #print (cmd)
    output = subprocess.check_output(cmd, shell=True, universal_newlines=True)
    return output


def load_json_db(db_file):
    """ Load the the data from a JSON file

    :param db_file: the JSON database file
    :returns a dictionary that contains the data
    """
    db = dict()
    with open(db_file, 'r') as f:
        db = json.load(f)
    return db


def save_json_db(db_file, db, indentation=4):
    """ Save the dictionary data to a JSON file

    :param db_file: the JSON database file
    :param db: the python dict struct
    :returns None
    """
    if not db:
        return
    print ("save_json_db: db file is " + db_file)
    #db["version"] = TOOL_VERSION
    #db["bomsh_hook_runcmd"] = g_bomsh_runcmd
    #db["bomsh_hook_cwd"] = g_bomsh_cwd
    try:
        f = open(db_file, 'w')
    except IOError as e:
        print ("I/O error({0}): {1}".format(e.errno, e.strerror))
        print ("Error in save_json_db, skipping it.")
    else:
        with f:
            json.dump(db, f, indent=indentation, sort_keys=True)


############################################################
#### End of helper routines ####
############################################################

def get_all_source_files_of_commits(commits):
    """
    Find all source files for a list of commits.
    :param commits: a list of git commit IDs
    returns a list of (commit_id, list-of-(afile, pre_checksum, post_checksum))
    """
    ret = []
    for commit in commits:
        afiles = get_commit_source_files(commit)
        ret.append( (commit, afiles) )
    verbose(json.dumps(ret, indent=4, sort_keys=True), LEVEL_4)
    return ret


def get_all_source_files_of_cve(cve, cve_db):
    """
    Find all source files for a CVE.
    :param cve_db: cve_db provides all the CVE commits, cve => list of commits mapping
    returns a list of (commit_id, list-of-(afile, pre_checksum, post_checksum))
    """
    commits = cve_db[cve]
    verbose("Get the source files of all commits for CVE: " + cve, LEVEL_3)
    return get_all_source_files_of_commits(commits)


def merge_all_source_files_of_cve(cve, cve_db):
    """
    Merge the source files of all commits for a CVE.
    return a new CVE DB, merging the same source file from all commits of a CVE.
           cve => { afile => (pre_checksum, post_checksum) }
    """
    result = {}
    commits = cve_db[cve]
    verbose("Merging all source files for cve: " + cve, LEVEL_3)
    for commit_id, afiles in commits:
        for afile, pre_checksum, post_checksum in afiles:
            # the list of commits is in reverse-chronological order, newer commit is kept if two commits touch the same source file.
            if afile in result:
                verbose("Warning: this source file " + afile + " already has newer commit. newer checksums: " + str(result[afile])
                        + " older checksums:" + str((pre_checksum, post_checksum)), LEVEL_4)
                if result[afile][0] != post_checksum:
                    verbose("Warning!! the two blob checksums are not adjacent: " + result[afile][0] + " " + post_checksum, LEVEL_3)
            else:
                result[afile] = (pre_checksum, post_checksum)
    return result


def parse_one_cve_commit(commit_str):
    """
    Parse a single CVE commit.
    :param commit_str: the commit message
    returns the commit ID and all the matched CVEs in a list.
    """
    result = re.findall(r'CVE-\d{4}-\d+', commit_str)
    #print(result)
    result = list(set(result))
    if commit_str[:7] == 'commit ':  # the first commit message contains the 'commit ' leading string
        commit_id = commit_str[7:47]
    else:  # all other commit messages starts with the 40-digit SHA1 checksum, which is the git commit ID.
        commit_id = commit_str[:40]
    #print(commit_id, result)
    return (commit_id, result)


def find_all_cve_commits():
    """
    Try to find all CVE commits by grep the commit message.
    returns a DB of {cve => list of git commit IDs} mapping
    """
    #cmd = 'git log --oneline --no-abbrev --grep "CVE-" || true'
    # cmd = 'git log --no-abbrev --grep "CVE-" --grep fix --all-match | egrep "(^commit |CVE-)" || true'
    cmd = 'git log --no-abbrev --grep "CVE-" || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    #print(output)
    if not output:
        return []
    commits = output.split("\n\ncommit ")
    commit_db = {}
    cve_db = {}
    for commit in commits:
        (commit_id, cve_result) = parse_one_cve_commit(commit)
        commit_db[commit_id] = cve_result
        for cve in cve_result:
            if cve in cve_db:
                cve_db[cve].append(commit_id)
            else:
                cve_db[cve] = [commit_id,]
    verbose(json.dumps(cve_db, indent=4, sort_keys=True), LEVEL_4)
    if args.verbose > 1:
        jsonfile = g_jsonfile
        if jsonfile[-5:] == ".json":
            jsonfile = jsonfile[:-5]
        afile = jsonfile + "-commitcves.json"
        save_json_db(afile, commit_db)
        print("All the commit => CVEs mappings are stored in the file " + afile)
        afile = jsonfile + "-cvecommits.json"
        save_json_db(afile, cve_db)
        print("All the CVE => commits mappings are stored in the file " + afile)
    return cve_db


def get_all_cve_source_files(cve_db):
    """
    Get the source files of all CVE commits.
    :param cve_db: it contains cve => list of commits mapping
    """
    cve_source_files_db = {}
    for cve in cve_db:
        result = get_all_source_files_of_cve(cve, cve_db)
        cve_source_files_db[cve] = result
    cve_source_files_merged_db = {}
    for cve in cve_source_files_db:
        result = merge_all_source_files_of_cve(cve, cve_source_files_db)
        cve_source_files_merged_db[cve] = result
    if args.verbose > 1:
        jsonfile = g_jsonfile
        if jsonfile[-5:] == ".json":
            jsonfile = jsonfile[:-5]
        afile = jsonfile + "-source-files.json"
        save_json_db(afile, cve_source_files_db)
        print("The source files and checksums of all the CVE commits are stored in the file " + afile)
        afile = jsonfile + "-merged-source-files.json"
        save_json_db(afile, cve_source_files_merged_db)
        print("The merged source files and checksums of all the CVEs are stored in the file " + afile)
    return cve_source_files_merged_db


def process_cve_source_files(cve_db):
    """
    Process all CVEs, associate CVEs with all relevant source file blob IDs
    :param cve_db: contains afile => (pre_blob_ids, post_blob_ids) mapping for all CVEs.
    returns a database of {git blob ID => CVElist/FixedCVElist } mapping
    """
    db = {}
    for cve in cve_db:
        commit_files = cve_db[cve]
        # convert from k:v mapping to (k,*v) list for use by associate_cve_to_commit_checksums
        source_files = [ (k, *v) for k,v in commit_files.items() ]
        verbose(cve + " associated blob_ids: " + str(source_files), LEVEL_1)
        associate_cve_to_commit_checksums(cve, source_files, db)
    return db


############################################################
#### End of CVE commit finding/processing routines ####
############################################################

def get_git_object_full_checksum(short_checksum):
    '''
    Convert gitref ID of short form to full-length 40-digit SHA1 checksum form.
    '''
    if short_checksum == '0' * len(short_checksum):
        return g_all_zero_checksum
    cmd = "git rev-parse " + short_checksum + ' || true'
    output = get_shell_cmd_output(cmd)
    return output.strip()


'''
Sample output from "git log" command:
7763472fe8fe42a1c830fcc9d35ca11fd9e6fcab Add support for C++ in Configurations/unix-Makefile.tmpl
:100644 100644 b610b5834dd8444a0e3b7ef920045ffb33de3c93 7f6caea17f643cc1c7a4306be0abb39f848d19bf M	Configurations/unix-Makefile.tmpl
'''

def get_all_git_blob_ids_for_afile(afile):
    """
    Get all the git blob IDs in a git repo for a file.
    returns a list in reverse-chronological order.
    """
    if not os.path.exists(afile):
        return []
    cmd = 'git log --raw --no-abbrev --oneline ' + afile + ' | grep "' + afile + '" || true'
    #print(cmd)
    output = get_shell_cmd_output(cmd)
    #print(output)
    if not output:
        return []
    lines = output.splitlines()
    ret = []
    for line in lines:
        if line[0] != ':':
            # it might be commit line
            continue
        tokens = line.split()
        blob_id = tokens[3]
        ret.append(blob_id)
    verbose(afile + " number of all git blob IDs: " + str(len(ret)), LEVEL_1)
    return ret


def is_interested_source_code_file(afile):
    """
    If a file is the source code file that we are interested.
    """
    tokens = afile.split(".")
    if len(tokens) > 1 and tokens[-1] in ("c", "cpp", "pl", "tmpl", "py", "s", "S"):
        # we care about C/C++/perl/template/python/assembly source code files
        return True
    return False


'''
The below is a merge commit in linux kernel git repo:
# git show 6aee4badd8126f3a2b6d31c5e2db2439d316374f
diff --cc arch/m68k/kernel/syscalls/syscall.tbl
index a00a5d0db602,2559925f1924..b911e0f50a71

# git describe a00a5d0db602 2559925f1924 b911e0f50a71
v5.5-rc3-1-ge8bb2a2a1d51:arch/m68k/kernel/syscalls/syscall.tbl
v5.5-rc1-11-gfddb5d430ad9:arch/m68k/kernel/syscalls/syscall.tbl
v5.5-5012-g6aee4badd812:arch/m68k/kernel/syscalls/syscall.tbl
'''

def get_commit_git_show_output_source_files(output):
    """
    Get all the source files in a commit's "git show" output.
    returns a list of (afile, pre_checksum, post_checksum)
    """
    ret = []
    lines = output.splitlines()
    print("here is the git show output: ", output)
    for line in lines:
        tokens = line.split()
        if "diff --" in line:
            afile = tokens[2]
            if len(tokens) > 3:
                bfile = tokens[3]
                if afile[2:] != bfile[2:]:
                    print("Warning: file path changed in this commit: " + afile + " => " + bfile)
            afile = afile[2:]  # drop the "a/" prefix from the file path
        elif "index " in line:
            (pre_checksum, post_checksum) = tokens[1].split("..")
            if "," in pre_checksum:  # this is probably a merge commit
                pre_checksum = pre_checksum.split(",")[-1]
            if is_interested_source_code_file(afile):
                ret.append( (afile, get_git_object_full_checksum(pre_checksum), get_git_object_full_checksum(post_checksum)) )
    return ret


def get_commit_source_files(commit):
    """
    Get all the source files in a git commit
    :param commit: the git commit ID
    """
    cmd = "git show " + commit + ' | egrep "(^diff --|^index )" || true'
    print(cmd)
    output = get_shell_cmd_output(cmd)
    #print(output)
    if not output:
        return []
    return get_commit_git_show_output_source_files(output)


def associate_cve_to_blob_id(cve, afile, blob_id, db, cvelist_field):
    '''
    Associate CVE to a list of git blob IDs.
    :param cve: the CVE ID
    :param afile: the git file name, relative path to git root dir
    :param blob_id: the git blob ID
    :param db: the CVE database to update
    :param cvelist_field: the name for the cvelist field in the CVE database
    '''
    if blob_id not in db:
        db[blob_id] = {cvelist_field: [cve,], "file_path": afile}
    else:
        entry = db[blob_id]
        if cvelist_field in entry:
            cvelist = entry[cvelist_field]
            cvelist.append(cve)
        else:
            entry[cvelist_field] = [cve, ]
        if "file_path" in entry:
            if entry["file_path"] != afile:
                # keep the existing file_path value
                print("Warning: two files for the same blob ID: " + blob_id, entry["file_path"], afile)
        else:
            entry["file_path"] = afile


def associate_cve_to_blob_ids(cve, afile, blob_ids, db, cvelist_field):
    '''
    Associate CVE to a list of git blob IDs.
    :param cve: the CVE ID
    :param afile: the git file name, relative path to git root dir
    :param blob_ids: a list of git blob IDs
    :param db: the CVE database to update
    :param cvelist_field: the name for the cvelist field in the CVE database
    '''
    for blob_id in blob_ids:
        associate_cve_to_blob_id(cve, afile, blob_id, db, cvelist_field)


def divide_git_blob_ids(checksums, pre_checksum, post_checksum):
    '''
    Divide all checksums into two lists: pre_blob_ids and post_blob_ids
    :param checksums: a list of all git blob IDs, should be in reverse-chronological order
    :param pre_checksum: the pre blob ID of the git commit that fixes the CVE
    :param post_checksum: the post blob ID of the git commit that fixes the CVE
    '''
    pre_blob_ids = [pre_checksum,]
    post_blob_ids = [post_checksum,]
    pre_index = checksums.index(pre_checksum) if pre_checksum in checksums else -1
    post_index = checksums.index(post_checksum) if post_checksum in checksums else -1
    if pre_index >= 0:
        pre_blob_ids = checksums[pre_index:]
    if post_index >= 0:
        post_blob_ids = checksums[:post_index+1]
    if g_all_zero_checksum in pre_blob_ids:  # Let's remove this all-zero blob ID
        pre_blob_ids = [checksum for checksum in pre_blob_ids if checksum != g_all_zero_checksum]
    if g_all_zero_checksum in post_blob_ids:  # Let's remove this all-zero blob ID
        post_blob_ids = [checksum for checksum in post_blob_ids if checksum != g_all_zero_checksum]
    verbose("divide into #pre_blob_ids, #post_blob_ids: " + str( (len(pre_blob_ids), len(post_blob_ids)) ), LEVEL_1)
    return (pre_blob_ids, post_blob_ids)


g_cvelist_field = "CVElist"
g_fixed_cvelist_field = "FixedCVElist"

def associate_cve_to_commit_checksums(cve, commit_files, db):
    '''
    Associate CVE to a list of git commit files (afile, pre_checksum, post_checksum)
    :param cve: the CVE ID
    :param commit_files: a list of (afile, pre_checksum, post_checksum)
    :param db: the CVE database to update
    '''
    for afile, pre_checksum, post_checksum in commit_files:
        checksums = get_all_git_blob_ids_for_afile(afile)
        pre_blob_ids, post_blob_ids = divide_git_blob_ids(checksums, pre_checksum, post_checksum)
        associate_cve_to_blob_ids(cve, afile, pre_blob_ids, db, g_cvelist_field)
        associate_cve_to_blob_ids(cve, afile, post_blob_ids, db, g_fixed_cvelist_field)


############################################################
#### End of git commit handling routines ####
############################################################

def is_all_zero_checksum(checksum):
    return checksum == '0' * len(checksum)


def get_file_path_for_blob_id(blob_id):
    '''
    Get file path for a single git blob ID.
    '''
    cmd = 'git describe --always ' + blob_id + ' || true'
    print(cmd)
    output = get_shell_cmd_output(cmd)
    #print(output)
    if not output:
        return ''
    tokens = output.strip().split(":")
    return tokens[1]


def get_file_path_for_blob_ids(blob_ids):
    '''
    Get file path for a list of git blob IDs.
    '''
    blob_ids_str = ' '.join(blob_ids)
    cmd = 'git describe --always ' + blob_ids_str + ' || true'
    print(cmd)
    output = get_shell_cmd_output(cmd)
    #print(output)
    if not output:
        return []
    lines = output.strip().splitlines()
    files = []
    for line in lines:
        tokens = line.split(":")
        files.append(tokens[1])
    return files


def get_file_path_for_cve_range(checksum1, checksum2):
    '''
    Get the file path for a CVE range of (checksum1, checksum2)
    '''
    afile = ''
    if is_all_zero_checksum(checksum1):
        afile = get_file_path_for_blob_id(checksum2)
    else:
        afile = get_file_path_for_blob_id(checksum1)
    return afile


def get_blob_ids_for_range(checksum1, checksum2):
    '''
    Get all the blob IDs for a file between checksum1 and checksum2, including the two checksums.
    checksum2 should be newer than checksum1.
    returns a list in chronological order.
    '''
    checksum = checksum1
    if is_all_zero_checksum(checksum1):
        afile = get_file_path_for_blob_id(checksum2)
    elif is_all_zero_checksum(checksum2):
        afile = get_file_path_for_blob_id(checksum1)
    else:
        files = get_file_path_for_blob_ids( (checksum1, checksum2) )
        if files[0] != files[1]:
            print("Warning: two checksums are not the same file:", (checksum1, files[0]), (checksum2, files[1]))
            return []
        afile = files[0]
    checksums = get_all_git_blob_ids_for_afile(afile)[::-1]
    index1 = 0
    if not is_all_zero_checksum(checksum1):
        index1 = checksums.index(checksum1)
    index2 = len(checksums) - 1
    if not is_all_zero_checksum(checksum2):
        index2 = checksums.index(checksum2)
    return checksums[index1 : index2 + 1]


def read_cve_range_file(afile):
    '''
    Read CVE blob_id ranges from a file.
    returns a DB containing all valid CVE blob_id ranges.
    '''
    contents = read_text_file(afile)
    lines = contents.splitlines()
    range_db = {}
    for line in lines:
        if not line or line[0] == '#':
            continue
        tokens = line.split()
        if len(tokens) != 3:
            continue
        cve = tokens[0]
        checksum1 = tokens[1]
        checksum2 = tokens[2]
        #print(cve, checksum1, checksum2)
        if not is_all_zero_checksum(checksum1) and not is_all_zero_checksum(checksum2):
            files = get_file_path_for_blob_ids(tokens[1:])
            if files[0] != files[1]:
                print("Warning: two checksums are not the same file:", (checksum1, files[0]), (checksum2, files[1]))
                continue
        if cve in range_db:
            range_db[cve].append( (checksum1, checksum2) )
        else:
            range_db[cve] = [ (checksum1, checksum2), ]
    return range_db


def process_read_cve_ranges(range_db):
    '''
    Process the read CVE blob_id ranges.
    :param range_db: contains CVE => list of (checksum1, checksum2) ranges
    returns a new DB containing all CVE blob_ids and all non-CVE blob_ids for source files.
    '''
    # First extract the file path for all the ranges.
    new_range_db = {}
    for cve in range_db:
        ranges = range_db[cve]
        cve_ranges = {}
        for checksum1, checksum2 in ranges:
            afile = get_file_path_for_cve_range(checksum1, checksum2)
            if afile in cve_ranges:
                cve_ranges[afile].append( (checksum1, checksum2) )
            else:
                cve_ranges[afile] = [ (checksum1, checksum2), ]
        new_range_db[cve] = cve_ranges
    # now merge the ranges for the same source file
    blobid_db = {}
    for cve in new_range_db:
        ranges = new_range_db[cve]
        range_blob_ids = {}
        for afile in ranges:
            cve_blob_ids = set()
            for checksum1, checksum2 in ranges[afile]:
                blob_ids = get_blob_ids_for_range(checksum1, checksum2)
                cve_blob_ids |= set(blob_ids)  # merge all the blob IDs for the same file
            all_blob_ids = get_all_git_blob_ids_for_afile(afile)
            non_cve_blob_ids = set(all_blob_ids) - cve_blob_ids
            range_blob_ids[afile] = (list(cve_blob_ids), list(non_cve_blob_ids))
        blobid_db[cve] = range_blob_ids
    if args.verbose > 1:
        jsonfile = g_jsonfile
        if jsonfile[-5:] == ".json":
            jsonfile = jsonfile[:-5]
        afile = jsonfile + "-range-blob-db.json"
        save_json_db(afile, blobid_db)
        print("All the CVE file => (cve_blob_ids, non_cve_blob_ids) mappings are stored in the file " + afile)
    return blobid_db


def process_cve_range_blob_ids(cve_blobid_db):
    """
    Process all CVEs, associate CVEs with all relevant source file blob IDs
    :param cve_blobid_db: contains afile => (cve_blob_ids, non_cve_blob_ids) mapping for all CVEs.
    returns a database of {git blob ID => CVElist/FixedCVElist } mapping
    """
    db = {}
    for cve in cve_blobid_db:
        file_blob_ids = cve_blobid_db[cve]
        for afile in file_blob_ids:
            cve_blob_ids, non_cve_blob_ids = file_blob_ids[afile]
            verbose(cve + " associated blob_ids: " + str(cve_blob_ids), LEVEL_2)
            associate_cve_to_blob_ids(cve, afile, cve_blob_ids, db, g_cvelist_field)
            associate_cve_to_blob_ids(cve, afile, non_cve_blob_ids, db, g_fixed_cvelist_field)
    return db


############################################################
#### End of CVE range handling routines ####
############################################################


def rtd_parse_options():
    """
    Parse command options.
    """
    parser = argparse.ArgumentParser(
        description = "This tool creates CVE database of git object to CVE mappings")
    parser.add_argument("--version",
                    action = "version",
                    version=VERSION)
    parser.add_argument('-j', '--jsonfile',
                    help = "the output JSON file to store the created CVE database")
    parser.add_argument('-r', '--range_of_vulnerable_cve',
                    help = "the text file to specify the CVE vulnerable ranges")
    parser.add_argument("-v", "--verbose",
                    action = "count",
                    default = 0,
                    help = "verbose output, can be supplied multiple times"
                           " to increase verbosity")

    # Parse the command line arguments
    args = parser.parse_args()

    #if not (args.raw_checksums_file):
    #if not (args.cve_db_file and args.raw_cve_commits_file):
    if False:
        print ("Please specify the CVE database file with -d option!")
        print ("Please specify the BOMSH raw checksum database file with -t option!")
        print ("")
        parser.print_help()
        sys.exit()

    global g_jsonfile
    if args.jsonfile:
        g_jsonfile = args.jsonfile

    print ("Your command line is:")
    print (" ".join(sys.argv))
    print ("The current directory is: " + os.getcwd())
    print ("")
    return args


def main():
    global args
    # parse command line options first
    args = rtd_parse_options()

    if args.range_of_vulnerable_cve:
        cve_ranges = read_cve_range_file(args.range_of_vulnerable_cve)
        range_db = process_read_cve_ranges(cve_ranges)
        cve_db = process_cve_range_blob_ids(range_db)
        save_json_db(g_jsonfile, cve_db)
        print("The created CVE database is stored in the file " + g_jsonfile)
        return

    # First find all CVE commits by grep commit messages, then find and merge all source files of those cve commits.
    # Then process all the CVEs, associate CVEs to all relevant source file blob IDs to create the CVE DB.
    cve_commits_db = find_all_cve_commits()
    cve_source_files_db = get_all_cve_source_files(cve_commits_db)
    cve_db = process_cve_source_files(cve_source_files_db)
    save_json_db(g_jsonfile, cve_db)
    print("The created CVE database is stored in the file " + g_jsonfile)
    return


if __name__ == '__main__':
    main()
